\documentclass[a4paper,11pt]{scrartcl}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{cite}
%\usepackage{showlabels}

\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes

\title{Seminararbeit Numerik}
\subtitle{Iterative Solvers - Algebraic Multi-grid}
\author{Bernd Schwarzenbacher, Daniel Herold}

\begin{document}
\maketitle
\tableofcontents
\pagebreak

\section{Motivation} \label{section:motiv}
\subsection{Iterative Solvers} \label{section:iter}
Iterative methods are used to solve big systems of linear equations in form of
matrices.
In practice the conjugate gradient (CG) method is the most often used iterative method for
the (often sparse and positive-definite) matrices arising from finite-element
discretization of partial differential equations.
This method is based on substituting the original problem by the fixed-point
equation:
$$b-Ax = 0 \iff x + \tau C^{-1} (b-Ax) = x$$
where $C$ is an invertible matrix.

From this formulation, one can immediately derive the
{\em Richardson Iteration}\/, which is a simple iterative method and is
used here to illustrate some essential aspects:

\begin{algorithm}
\caption{Richardson Iteration}
\begin{algorithmic}
  \STATE \text{start value} \: $x_{0}$
  \FOR{$j = 0, 1, 2, \dots$}
    \STATE $x_{j+1} = x_{j} + C^{-1} (b - Ax_{j})$
  \ENDFOR
\end{algorithmic}
\end{algorithm}

The matrix $C$ denotes the so-called preconditioner, which is used to
improve the condition of the problem to obtain faster convergence speed.
It should fulfill two major conditions:
\begin{itemize}
\item cheap matrix-vector multiplication with $C^{-1}$
\item good approximation of $A$
\end{itemize}

A first approach for $C$ is to take the diagonal of $A$ as the
preconditioner:
$$C = diag(A)$$
This constitutes the {Jacobi method}:
$$ x_j^{new} = x_j^{old} + \frac{1}{A_{jj}} \left(b_{j} -
               \sum_{k} A_{jk} x_k^{old}\right)
               \qquad j = 1 \dots n$$
It can be altered to use the newly computed entries as well. This leads to
the {Gauss-Seidel method}, which is described in more detail in
chapter~\ref{section:gs}.
\cite{iterative}


\subsection{Algebraic Multi-Grid}
The Multi-grid (MG) method now seeks to combine the smoothing of high
frequency errors by the Jacobi or Gauß-Seidel method with a second method which
better addresses lower frequency errors.

Therefore a prolongation matrix $P$ is introduced, which for a basic example
can look like:
$$P =
\begin{pmatrix}
  1 & 0 & 0 & 0 \\
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 1 \\
\end{pmatrix}
$$

Now there are several approaches to come up with a prolongation matrix.
The Algebraic Multi-grid (AMG) method we adopted uses only the information
in the matrix $A$ to compute the coarsening and then the projection matrix.
It does so by weighting the relative strength of connecting vertices to
determine which vertices should collapse on the next level.

Once the prolongation matrix is computed it is used to compute the update in a
step of an iterative method similar to the preconditioners described in
chapter~\ref{section:iter}.
First the transposed prolongation matrix projects the residuum on a
coarser space, where the problem can be solved directly and then the computed
update gets prolongated back onto the fine space by multiplication with $P$.
An iterative step with two levels is thus described by:
$$x^{new} = x^{old} +\underbrace{P}_\text{refine}\cdot \underbrace{(P^{T} A P)^{-1}}_\text{solve the coarsed problem}\cdot \underbrace{P^{T}}_\text{coarse the residuum} (b - A x^{old})$$

The combination with the Gauß-Seidel method is provided by solving the problem
recursively on multiple levels and applying a forward and backward smoothing
step on each level. Additionally a transposed matrix vector multiplication
is needed to project onto the coarse space.
See Listing~\ref{lst:mult} for more details on the AMG algorithm.
Those three points are subject to performance improvements by parallelization
which is the main point we address.
\cite{multigrid}

\section{Sparse Matrices}
In our case the sparse matrices are stored in the Compressed Row Storage (CRS)
format. The commands {\em firsti[k]}\/ and {\em lasti[k]}\/ provide the
position of the first and last entry of the row k in the index vector.
An iteration through one row is very easy to provide:

\lstset{language=C++, numbers=none, captionpos=b,
        caption={Row-iteration of sparsematrices }}
\begin{lstlisting}
for (int j = firsti[k]; j < lasti[k]; ++j)
  {...}
\end{lstlisting}

The difficulty with sparse matrices is the iteration among any other order as
the next section will show.

\section{Matrix Vector Multiplication}
The multiplication of a sparse matrix and a given vector is very simple.
The entry k of the output vector is given by the summation of the products
of the matrix entries in the row k with the entries of the vector. In other
words, one entry of the output vector depends only on one row of the matrix
and vice versa.

One of the difficulties of parallelization is given by multiple writing
performances on a single variable at the same time. In the case of a simple
matrix vector multiplication this is no big deal to handle: each row has to be
executed by a single thread, there is no limitation in which order which thread
calculates its vector entry.

\lstset{language=C++, numbers=left, captionpos=b,
        caption={Matrix vector multiplication with parallel for section.}}
\begin{lstlisting}
void MultAddParallel w(double s, const BaseVector & x, BaseVector & y)
{
#pragma omp parallel for
    for (int i = 0; i < this->Height(); ++i) {
      int first = firsti [i];
      int last  = firsti [i+1];

      for (int j = first; j < last; ++j) {
        y(i) += s * data[j] * x(colnr[j]);
      }
    }
  }

\end{lstlisting}

\subsection{Transposed Matrix Vector Multiplication}\label{section:trans}
To provide the multiplication of a vector with the transposed matrix, a
sequential algorithm is written in a similar way.

\lstset{language=C++, numbers=none, captionpos=b,
        caption={Transposed vector multiplication algorithm.}}
\begin{lstlisting}
    for (int i = 0; i < this->Height(); ++i) {
      int first = firsti [i];
      int last  = firsti [i+1];

      for (int j = first; j < last; ++j) {
        y(colnr[j]) += s * data[j] * x(i);
      }
    }
\end{lstlisting}

The difference to the previous algorithm is due to the dependencies of a single
output vector entry. It is calculated by iterating through the entries
of one matrix column. This is not efficient due to the CRS format.
Therefore the algorithm has to iterate first through the matrix rows.
The access of distinct entries per row is no problem for the sequential
code, but simple parallelization causes issues.
Concurrent reading and writing operations to one variable can lead to incorrect
stored values as shown in figure~\ref{figure:parallelwriting}. Two
thread are trying to write at the same variable at almost the same time. The
second thread starts his procedure before thread 1 has finished and overwrites the
result of the first thread.

\begin{figure}[ht]
\includegraphics{graphic/parallel_writing_problem.pdf}
\caption{Problem with Parallelization of Writing Processes}
\label{figure:parallelwriting}
\end{figure}

\begin{samepage}
Some approaches to solve this problem are:
\begin{itemize}
\item atomic writing process (easy to program, but very inefficient)
\item atomic writing process with dynamic thread partitioning
\item atomic writing process with static thread separation
\item matrix coloring without any atomic processes
\end{itemize}
\end{samepage}

All those solutions efficiency depend on the number of entries in the matrix.
Less entries and a larger matrix result in higher amounts of calculations per
time and thus are suited for parallel algorithms.
Whereas a sequential algorithm fares better than a parallel solution for
matrices with a high amount of nonzero elements for this problem.

The \textbf{atomic} version is very straight forward. The idea is to avoid the
problem shown in figure~\ref{figure:parallelwriting} by making the operations
atomic. This means that a thread blocks the address it accesses till it is
finished with its read-modify-write operation.

\lstset{language=C++, numbers=none, captionpos=b,
        caption={Atomic section in the transposed vector multiplication.}}
\begin{lstlisting}
#pragma omp atomic
          fy(colnr[j]) += s * data[j] * fx(i);
\end{lstlisting}

This part is the bottleneck of the algorithm, because every thread has to has
pass this line and they block each other.

The \textbf{dynamic thread partitioning} solves another problem of this code.
{\em OMP for}\/ (as used in the atomic version) splits the for-loop into a
evenly distributed amount of iterations for each thread beforehand. Due to
differing amount of calculations each thread has to perform, some may finish
faster than other thread. This relates to the specific structure of matrices
resulting in a finite element discretization, the number of nonzero elements
per row can vary significantly.

Instead it is possible to split the jobs dynamically. Faster threads are
assigned new iterations while slower thread still work on the old iterations.
The dynamic assignment produces some overhead, which can be mitigated by
assigning chunks of iterations.

\lstset{language=C++, numbers=none, captionpos=b,
        caption={Dynamic for loop.}}
\begin{lstlisting}
#pragma omp for schedule (dynamic, 100)
\end{lstlisting}

The \textbf{balancing option} performs a similar assignment of iterations, but
does so before the for-loop starts instead of a dynamic assignment. The work
for the calculation of each row is approximated with the number of nonzero
elements in this row and split the loop accordingly. (see listing~\ref{lst:tranbal}



Every method still has the same problem with the atomic operation in its
essential part. We can avoid this by "\textbf{coloring}" each row of the matrix.
Rows are assigned to the same color if they don't write on the same entries.
Parallelization of this group is therefore possible, as no conflicting
concurrent operations will occur. Thus it is possible to iterate through the
colors sequentially and parallelize each color.

Not writing at the same entry is equivalent to not sharing any columns with
nonzero elements. Figure~\ref{figure:coloring} shows an example of the
coloring process.


\begin{figure}[ht]
\includegraphics[width=0.32\textwidth]{graphic/coloringT2.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT3.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT4.eps}
\includegraphics[width=0.32\textwidth]{graphic/coloringT5.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT6.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT7.eps}
\includegraphics[width=0.32\textwidth]{graphic/coloringT8.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT9.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT10.eps}
\caption{Coloring of a Simple Matrix}\label{figure:coloring}
\end{figure}

This is easy to perform for small matrices. To cope with larger matrices we
iterate through the matrix for every color and store the position of blocked
nonzero elements for the specific color in a mask.
Each row is compared to the mask and if no nonzero entry of the row interferes
with the already blocked entries, the row is added to the current color and the
mask gets updated accordingly.
As long as there are uncolored rows of the matrix new colors will be used.
To further optimize this we use a mask for 32 colors at once.
(see listing~\ref{lst:trancol}
and~\ref{lst:tranmul})



\section{Gauß-Seidel Method} \label{section:gs}
As mentioned in chapter~\ref{section:motiv} we use two Gauß-Seidel steps on
each level. The Gauß-Seidel method is similar to the Jacobi method.
Both methods need to compute the vector $Ax$. The difference is, that the
Gauß-Seidel method also uses the new information obtained by previous
computations of entries.
In particular a new entry is given by:
$$ x_j^{new} = x_j^{old} + \frac{1}{A_{jj}} \left(b_{j} -
  \sum_{k \in K^{new}}A_{jk} x_k^{new} - \sum_{k \in K^{old}}A_{jk} x_k^{old}
  \right) \qquad j = 1 \dots n$$
$K^{new}$ denotes the index set of newly calculated entries
of $x$ and $K^{old}$ denotes the index set of old entries.
\cite{iterative}

The calculation order in the forward Gauß-Seidel precondition is arbitrary,
but to achieve a symmetric preconditioner the backward smoothing needs to be
in exact reverse order.

Entries of the output vector $x$ must not be calculated concurrently if they
depend on each other. As in chapter~\ref{section:trans} described, this would
cause incorrect entries. The problems with parallelization are also very
similar to chapter~\ref{section:trans}, as are the ideas to cope with them.

A coloring algorithm can be used as well and is similar in its implementation.
As with the transposed matrix vector multiplication we group the rows which
may run parallel without influencing each other.

\subsection{Coloring}
To calculate the product $A_jx$ which modifies the entry $x_j$ (where $A_j$
denotes the j-th row of the matrix $A$), for every $k$ with $A_{jk} \neq 0$,
$x_k$ is used. According to this observation, every row $A_j$, which should be
added to an existing color must not have any nonzero elements on the positions $A_{jk}$
for all $k$ indices of rows marked with this color.

Instead of testing all remaining rows for this property, we use that $A$ is
symmetric: $A_{kj} = 0 \Leftrightarrow A_{jk} = 0$. Thus we only check
the specific row we added to one color to determine which rows can also
relate to it.

The main advantage of this method is, that the coloring process has to be
done only once, whereas it is used by GSSmooth and GSSmoothBack as well.
Also the order of colors is stored by the process itself.

For a small matrix the coloring process is demonstrated in
figure~\ref{figure:coloringGS}.

\begin{figure}
\includegraphics[width=0.32\textwidth]{graphic/coloringGS1.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringGS4.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringGS7.eps}
\includegraphics[width=0.32\textwidth]{graphic/coloringGS8.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringGS9.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringGS10.eps}
\caption{Coloring of a Simple Matrix for the Gauß-Seidel Algorithm.}
\label{figure:coloringGS}
\end{figure}

\subsection{Speed Improvement}
As mentioned, the AMG preconditioner is a combination of the Gauß-Seidel
preconditioner and intermediate projections onto coarser spaces
by multiplying with the transposed prolongation matrix. These steps are
executed alternately until the matrix is small enough to invert it directly.
Then the correction is refined to its original size and backwards Gauß-Seidel is
used in between these steps.
This sums up to one step of the AMG-Preconditioner. Many of such steps are
executed as a part of the CG method until the solution is sufficiently exact.
(see listing~\ref{lst:mult})

Speed improvements by parallelization are possible within Gauß-Seidel steps
and the transposed matrix vector multiplication. The Gauß-Seidel coloring has to
be done separately for each level before the CG method
(see listing~\ref{lst:jacobi}), but can be used again
in each CG iteration (listing~\ref{lst:gss} and~\ref{lst:gssb}).
For the transposed matrix vector multiplication a balancing algorithm is used.

To measure and visualize the time of these steps a trace explorer can be used.
{\em Vite\footnote{http://vite.gforge.inria.fr}}\/ is a tool to visualize
execution traces in Pajé or OTF format for debugging and profiling parallel or
distributed applications.
The work of every thread is shown in a single row. Sections in the code have
different but specific colors, so it is obvious which parts of the algorithm
took most of the computing time and how the work is distributed among the
threads.

Figure~\ref{figure:sequ} shows a CG method with sequential AMG, whereas
figure~\ref{figure:par} shows it with parallel AMG\@.
Only one thread is working in the sequential version compared to 24 threads in
the parallel.

In figure~\ref{figure:par} and~\ref{figure:step} the red blocks indicate a
thread of forward Gauß-Seidel smoothing, the purple ones are projections and
prolongations and the green ones indicate backward Gauß-Seidel smoothing.
In figure~\ref{figure:step} a single step of the AMG is shown. The large matrix
needs more time to perform a Gauß-Seidel step.
This is indicated by the big red blocks at the beginning. Only one color is
calculated at the same time, but its rows can be processed by several threads
simultaneously.

\begin{figure}
    \includegraphics[width=1\textwidth]{seq.png}
    \caption{AMG sequential}\label{figure:sequ}
\end{figure}

\begin{figure}
    \includegraphics[width=1\textwidth]{iterations_trace.png}
    \caption{AMG parallel}\label{figure:par}
\end{figure}

\begin{figure}
    \includegraphics[width=1\textwidth]{undistributed_coloring_gs.png}
    \caption{AMG single parallel recursion}\label{figure:step}
\end{figure}

Due to the matrix being stored split in half on to the specific structure of
the {\em Vector}\/ server, its two nodes have different speed of writing and
reading at parts of the matrix.
Depending on the tread to node correspondence, some colors are faster
calculated by the first half of threads, and others by the other half.
This emerges from the coloring process. When iterating over the matrix rows,
the colors tend to cluster, as it is unlikely that rows have columns in common.
To mitigate this effect we iterate with an offset over the matrix rows in the
coloring process.

A comparison of figure~\ref{figure:step} and figure~\ref{figure:stepdis} shows
that most of the gray spaces between the blocks can be eliminated by minimizing the
correspondence of rows and colors. (see listing~\ref{lst:jacobi})

\begin{figure}
    \includegraphics[width=1\textwidth]{one_iteration.png}
    \caption{AMG single parallel recursion with distribution}\label{figure:stepdis}
\end{figure}

\pagebreak

\section{Code}

\lstset{language=C++, numbers=left, captionpos=t, breaklines=true,
        caption={Static balancing in the transposed vector multiplication.},label=lst:tranbal}
\begin{lstlisting}
// The separation of the matrix is calculated based on the number
// of non-zero elements in the matrix. Each thread has to do almost
// the same amount of work.
void TranMultBalance (double s, const BaseVector & x, BaseVector & y) const
{
  FlatVector<double> fx = x.FV<double> ();
  FlatVector<double> fy = y.FV<double> ();

  int height = this->Height();

  Array<int> thread_separation;
#pragma omp parallel
  {
    int num_threads = omp_get_num_threads();

#pragma omp single
    {
      thread_separation = Array<int>(num_threads+1);
      // load for one thread
      int separation_step = ceil(this->nze / num_threads);

      // find section of rows in the matrix for each thread
      int thread_i = 1;
      for (int row = 0; row < height; ++row)
      {
        if (firsti[row] >= thread_i * separation_step)
        {
          thread_separation[thread_i] = row;
          ++thread_i;
        }
      }
      thread_separation[0] = 0;
      thread_separation[num_threads] = height;
    }

// parallelization of the transposed matrix-vector-multiplication based on the
// previous calculated thread separation
#pragma omp for
    for (int thread_i = 1; thread_i <= num_threads; ++thread_i)
    {
      for (int i = thread_separation[thread_i-1];
            i < thread_separation[thread_i]; ++i)
      {
        int first = firsti [i];
        int last  = firsti [i+1];

        for (int j = first; j < last; ++j)
        {
#pragma omp atomic
          fy(colnr[j]) += s * data[j] * fx(i);
        }
      }
    }
  }
}

\end{lstlisting}

\lstset{language=C++, numbers=left, captionpos=t,
        caption={Coloring of a matrix for transposed vector multiplication},
        label=lst:trancol}
\begin{lstlisting}
void Coloring ()
{
  int height = this->Height();
  int width = this->Width();

  Array<int> row_color(height);  // storage of the colors for each row
  row_color = -1;

  int maxcolor = 0;
  int basecol = 0;

  Array<unsigned int> mask(width);  // 32-bit masks for each column

  int found = 0;

  do
  {
    mask = 0;

    for (int row = 0; row < height; ++row)
    {
      // next row if row already has a color
      if (row_color[row] >= 0) continue;

      int first = firsti [row];
      int last  = firsti [row+1];

      unsigned check = 0;

      // check is the union of all mask bits corresponding
      // to nze's in the row
      for (int i = first; i < last; ++i)  // iterate over entries of the row
        check |= mask[colnr[i]];


      // find next free color
      if (check != UINT_MAX)
      {
        found++;
        unsigned checkbit = 1;
        int color = basecol;
        while (check & checkbit)
        {
          color++;
          checkbit *= 2;
        }

        row_color[row] = color;  // set color to current row
        if (color > maxcolor) maxcolor = color;

        for (int i = first; i < last; ++i)
          mask[colnr[i]] |= checkbit;  // update mask
      }
    }

    basecol += 8*sizeof(unsigned int);
  }
  while (found < height);

  // count rows per color
  Array<int> cntcol(maxcolor+1);
  cntcol = 0;
  for (int row = 0; row < height; ++row)
    ++cntcol[row_color[row]];

  coloring_ = table<int>(cntcol);

  cntcol = 0;
  // construct mapping of rows to colors for the iteration
  for (int row = 0; row < height; ++row)
    coloring_[row_color[row]][cntcol[row_color[row]]++] = row;

  std::cout << "needed " << maxcolor+1 << " colors" << std::endl;
}

\end{lstlisting}

\lstset{language=c++, numbers=left, captionpos=t,
        caption={transposed vector multiplication by coloring a matrix},
        label=lst:tranmul}
\begin{lstlisting}
void TranMultAdd4 (double s, const BaseVector & x, BaseVector & y)
{
  FlatVector<double> fx = x.FV<double> ();
  FlatVector<double> fy = y.FV<double> ();

  Coloring();

#pragma omp parallel
  {
    // iterate sequential over colors
    for (auto color : coloring_)
    {
      // iterate parallel over corresponding rows
#pragma omp for
      for (int i = 0; i < color.Size(); ++i)
      {
        int first = firsti [color[i]];
        int last  = firsti [color[i]+1];

        for (int j = first; j < last; ++j)
        {
          fy(colnr[j]) += s * data[j] * fx(color[i]);
        }
      }
    }
  }
}


\end{lstlisting}

\lstset{language=C++, numbers=left, captionpos=t, breaklines=true,
        caption={Multiplication Method of the AMG Preconditioner},label=lst:mult}
\begin{lstlisting}
void my_AMG_H1 :: Mult (const ngla::BaseVector & b, ngla::BaseVector & x) const
{
  // direct solve at lowest level
  if (inv) {
    x = (*inv) * b;
    return;
  }

  auto residuum = pmat->CreateVector();
  auto coarse_x = coarsemat->CreateVector();
  auto coarse_residuum = coarsemat->CreateVector();

  x = 0;
  jacobi->GSSmooth (x, b);  // one Gauss-Seidel step

  if (recAMG) {
    residuum = b - (*pmat) * x;
    coarse_residuum = ngla::Transpose (*prol) * residuum;  // coarsening

    // recursive solving of coarser problem
    recAMG->Mult(coarse_residuum, coarse_x);

    x += (*prol) * coarse_x;  // prolongate back to fine grid
  }

  jacobi->GSSmoothBack (x, b);  // Gauss-Seidel back
}
\end{lstlisting}



\lstset{language=c++, numbers=left, captionpos=t,
        caption={Gauß-Seidel coloring and balancing},label=lst:jacobi}
\begin{lstlisting}

void JacobiPrecond<TM,TV_ROW,TV_COL> :: Coloring ()
{
  int height = mat.Height();
  int width = mat.Width();

  Array<int> row_color(height);  // storage of colors for each row
  row_color = -1;

  int maxcolor = 0;
  int basecol = 0;

  Array<unsigned int> mask(width);  // 32-bit masks for each column

  int found = 0;

  int num_threads = task_manager->GetNumThreads();

  do
  {
    mask = 0;

    int block_size = ceil((double)height / (double)num_threads);

    // iterate over blocks to get rows of the whole matrix
    // for every color
    for (int block_row = 0; block_row < block_size; ++block_row)
    {
      for (int threadi = 0; threadi < num_threads; ++threadi)
      {
        int row = block_row + threadi * block_size;
        if (row >= height) break;

        // next row if row already has a color
        if (row_color[row] >= 0) continue;

        int first = mat.firsti [row];
        int last  = mat.firsti [row+1];

        unsigned check = 0;

        // check is the union of all mask bits corresponding
        // to nze's in the row
        for (int i = first; i < last; ++i)  // iterate over entries of the row
          check |= mask[mat.colnr[i]];

        // find the next possible color
        if (check != UINT_MAX)
        {
          found++;
          unsigned checkbit = 1;
          int color = basecol;
          while (check & checkbit)
          {
            color++;
            checkbit *= 2;
          }

          row_color[row] = color;  // set the color to the current row
          if (color > maxcolor) maxcolor = color;

          mask[row] |= checkbit;  // update the mask
        }
      }
    }

    basecol += 8*sizeof(unsigned int);
  }
  while (found < height);

  Array<int> cntcol(maxcolor+1);
  cntcol = 0;
  for (int row = 0; row < height; ++row)
    ++cntcol[row_color[row]];  // count rows per color

  coloring_ = Table<int>(cntcol);

  cntcol = 0;
  // construct colors to row mapping
  for (int row = 0; row < height; ++row)
    coloring_[row_color[row]][cntcol[row_color[row]]++] = row;

  balancing_.SetSize (maxcolor+1);
  for (auto c : Range (balancing_))
  {
    balancing_[c].Calc (coloring_[c].Size(),
      [&] (int bi)
      {
        return 5 + mat.GetRowIndices(coloring_[c][bi]).Size();
      });
  }

  std::cout << "needed " << maxcolor+1 << " colors" << std::endl;
}

\end{lstlisting}

\lstset{language=c++, numbers=left, captionpos=t,
        caption={Gauß-Seidel Smooth},label=lst:gss}
\begin{lstlisting}
void JacobiPrecond<TM,TV_ROW,TV_COL> ::
GSSmooth (BaseVector & x, const BaseVector & b) const
{
  FlatVector<TV_ROW> fx = x.FV<TV_ROW> ();
  const FlatVector<TV_ROW> fb = b.FV<TV_ROW> ();

  // iterate sequential over colors
  for (int j = 0; j < this->coloring_.Size(); ++j)
  {
    auto color = this->coloring_[j];
    // task-manager based parallelization
    ParallelFor( this->balancing_[j],
      [this, fx, fb, color] (int i)
      {
        int row = color[i];
        if (!this->inner || this->inner->Test(row))
        {
          TV_ROW ax = mat.RowTimesVector (row, fx);
          fx(row) += invdiag[row] * (fb(row) - ax);
        }
      }, 4);
  }
}
\end{lstlisting}

\lstset{language=c++, numbers=left, captionpos=t,
        caption={Gauß-Seidel SmoothBack},label=lst:gssb}
\begin{lstlisting}
void JacobiPrecond<TM,TV_ROW,TV_COL> ::
GSSmoothBack (BaseVector & x, const BaseVector & b) const
{
  FlatVector<TV_ROW> fx = x.FV<TV_ROW> ();
  const FlatVector<TV_ROW> fb = b.FV<TV_ROW> ();

  // iterate sequential over colors
  for (int j = this->coloring_.Size()-1; j >= 0; --j)
  {
    auto color = this->coloring_[j];

    // task-manager based parallelization
    ParallelFor( this->balancing_[j],
      [this, fx, fb, color] (int i)
      {
        int row = color[i];
        if (!this->inner || this->inner->Test(row))
        {
          TV_ROW ax = mat.RowTimesVector (row, fx);
          fx(row) += invdiag[row] * (fb(row) - ax);
        }
      }, 4);
  }
}
\end{lstlisting}


\bibliography{doc}{}
\bibliographystyle{plain}
\end{document}
