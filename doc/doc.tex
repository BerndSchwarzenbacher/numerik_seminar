\documentclass[a4paper,11pt]{scrartcl}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{cite}

\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes

\title{Seminararbeit Numerik}
\subtitle{Iterative Solvers - Algebraic Multigrid}
\author{Bernd Schwarzenbacher, Daniel Herold}

\begin{document}
\maketitle
\tableofcontents
\pagebreak

\section{Motivation}
To solve big systems of linear equations the conjugate gradient method is the
most used iterative method. For faster convergence speed the condition of the
problem is further improved with a so-called preconditioner.
The original problem is therefore substituted by the preconditioned problem:
$$b-Ax = 0 \iff x + \tau C^{-1} (b-Ax) = x$$
Where the matrix $C$ denotes the preconditioner.

The Algebraic Multigrid (AMG) method combines several bad preconditioners to
obtain a good preconditioner in total. It belongs to the family of general
Multigrid (MG) methods which recursively coarsen the original matrix and
project the problem on this coarser spaces by use of a prolongation matrix.
On the coarser levels, low frequency errors in the solution are corrected
faster. The AMG method uses only the information in the system matrix to
compute the coarsening and further the projection matrix. It does so by
weighting the relative strength of connecting vertices to determine which
vertices should collapse on the next level.
\cite{numerik} \cite{numpde}

In the application of the AMG preconditioner we use one forward and one
backward Gauß-Seidel preconditioner per level. Additionally a
transposed matrix vector multiplication is needed to project onto the
coarse space. See Listing~\ref{lst:mult} for more details on the algorithm.
Those three points are subject to performance improvements by parallelization.

\section{Sparsematrices}
Most matrices are stored in the Compressed Column Storage (CCS) format.
The commands {\em firsti[k]} and {\em lasti[k]} provide the position of the
first and last entry of the row k in the indexvector. One
iteration through one row ist very easy to provide.

\lstset{language=C++, numbers=none, captionpos=b,
        caption={Row-iteration of sparsematrices }}
\begin{lstlisting}
for (int j = firsti[k]; j < lasti[k]; ++j)
      {...}

\end{lstlisting}

The difficulty with sparsematrices is the iteration among any other order as
the next section will show.

\section{Matrix Vector Multiplication}
The multiplication of a sparsematrix and a given vector is very simple.
The entry k of the output vector is given by the summation of the products
of the matrix entries in the row k with the entries of the vector. In other
words, one entry of the outcoming vector depends only on one row of the matrix
and vice versa.

One of the difficulties of parallelization is given by multiple writing
performances on a single variable at the same time. In the case of a simple
matrix vector multiplication this is no big deal to handle: each row has to be
executed by a single task, there is no limitation in which order which task
calculates its vector entry.

\lstset{language=C++, numbers=left, captionpos=b,
        caption={Matrix vector multiplication with parallel for section.}}
\begin{lstlisting}
void MultAddParallel w(double s, const BaseVector & x, BaseVector & y)
{
#pragma omp parallel for
    for (int i = 0; i < this->Height(); ++i) {
      int first = firsti [i];
      int last  = firsti [i+1];

      for (int j = first; j < last; ++j) {
        y(i) += s * data[j] * x(colnr[j]);
      }
    }
  }

\end{lstlisting}

\subsection{Transposed Matrix Vector Multiplication}\label{section:trans}
To provide the multiplication of a vector with the transposed matrix, a
sequential algorithm is written in a similar way.

\lstset{language=C++, numbers=none, captionpos=b,
        caption={Transposed vector multiplication algorithm.}}
\begin{lstlisting}
    for (int i = 0; i < this->Height(); ++i) {
      int first = firsti [i];
      int last  = firsti [i+1];

      for (int j = first; j < last; ++j) {
        y(colnr[j]) += s * data[j] * x(i);
      }
    }
\end{lstlisting}

The difference to the previous algorithm is due to the dependencies of a single
output vector entry. It is calculated by iterating through the entries
of one matrix column. This is not efficient due to the CCS format.
Therefore the algorithm has to iterate first through the matrix rows.
The access of distinct entries per row is no problem for the sequential
code, but simple parallelization causes issues.
Concurrent reading and writing operations to one variable can lead to incorrect
stored values as shown in figure~\ref{figure:parallelwriting}. Two
tasks are trying to write at the same variable at almost the same time. The
second task starts his procedure before task 1 has finished and overwrites the
result of the first task.

\begin{figure}[ht]
\includegraphics{graphic/parallel_writing_problem.pdf}
\caption{Problem with Parallelization of Writing Processes}\label{figure:parallelwriting}
\end{figure}

\begin{samepage}
Some ideas to solve this problem are:
\begin{itemize}
\item atomic writing process (easy to program, but very inefficient)
\item atomic writing process with dynamic task partioning
\item atomic writing process with static thread seperation
\item matrix coloring without any atomic processes
\end{itemize}
\end{samepage}

All those solutions efficiency depend on the number of entries in the matrix.
Less entries and a larger matrix result in higher amounts of calculations per
time and thus are suited for parallel algorithms.
Whereas a sequential algorithm fares better than a parallel solution for
matrices with a high amount of nonzero elements for this problem.

The \textbf{atomic} version is very straight forward. The idea is to avoid the
problem shown in figure~\ref{figure:parallelwriting} by making the operations
atomic. This means that a task blocks the address it accesses till it is
finished with its read-modify-write operation.

\lstset{language=C++, numbers=none, captionpos=b,
        caption={Atomic section in the transposed vector multiplication.}}
\begin{lstlisting}
#pragma omp atomic
          fy(colnr[j]) += s * data[j] * fx(i);
\end{lstlisting}

This part is the bottleneck of the algorithm, because every task has to has
pass this line and they block each other.

The \textbf{dynamic task partitioning} solves another problem of this code.
{\em OMP for} (as used in the atomic version) splits the for-loop into a
evenly distributed amount of iterations for each task beforehand. Due to
differing amount of calculations each task has to perform, some may finish
faster than other tasks. This relates to the specific structure of matrices
resulting in a finite element discretization, the number of nonzero elements
per row can vary significantly.

Instead it is possible to split the jobs dynamically. Faster threads are
assigned new iterations while slower tasks still work on the old iterations.
The dynamic assignment produces some overhead, which can be mitigated by
assigning chunks of iterations.

\lstset{language=C++, numbers=none, captionpos=b,
        caption={Dynamic for loop.}}
\begin{lstlisting}
#pragma omp for schedule (dynamic, 100)
\end{lstlisting}

The \textbf{balancing option} performs a similar assignment of iterations, but
does so before the for-loop starts instead of a dynamic assignment. The work
for the calculation of each row is approximated with the number of nonzero
elements in this row and split the loop accordingly.

\lstset{language=C++, numbers=left, captionpos=b,
        caption={Static balancing in the transposed vector multiplication.}}
\begin{lstlisting}
void TranMultBalance (double s, const BaseVector & x, BaseVector & y) const
  {
    FlatVector<double> fx = x.FV<double> ();
    FlatVector<double> fy = y.FV<double> ();

    int height = this->Height();

    Array<int> thread_seperation;
#pragma omp parallel //shared(thread_seperation)
    {
      int num_threads = omp_get_num_threads();

#pragma omp single
      {
        thread_seperation = Array<int>(num_threads+1);
        int seperation_step = ceil(this->nze / num_threads);

        int thread_i = 1;
        for (int row = 0; row < height; ++row)
        {
          if (firsti[row] >= thread_i * seperation_step)
          {
            thread_seperation[thread_i] = row;
            ++thread_i;
          }
        }
        thread_seperation[0] = 0;
        thread_seperation[num_threads] = height;
      }

#pragma omp for
      for (int thread_i = 1; thread_i <= num_threads; ++thread_i)
      {
        for (int i = thread_seperation[thread_i-1];
             i < thread_seperation[thread_i]; ++i)
        {
          int first = firsti [i];
          int last  = firsti [i+1];

          for (int j = first; j < last; ++j)
          {
#pragma omp atomic
            fy(colnr[j]) += s * data[j] * fx(i);
          }
        }
      }
    }
  }

\end{lstlisting}

Every method still has the same problem with the atomic operation in its
essential part. We can avoid this by "\textbf{coloring}" each row of the matrix.
Rows are assigned to the same color if they don't write on the same entries.
Parallelization of this group is therefore possible, as no conflicting
concurrent operations will occur. Thus it is possible to iterate through the
colors sequentially and parallelize each color.

Not writing at the same entry is equivalent to not sharing any columns with
nonzero elements. Figure~\ref{figure:coloring} shows an example of the
coloring process.


\begin{figure}[ht]
\includegraphics[width=0.32\textwidth]{graphic/coloringT2.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT3.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT4.eps}
\includegraphics[width=0.32\textwidth]{graphic/coloringT5.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT6.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT7.eps}
\includegraphics[width=0.32\textwidth]{graphic/coloringT8.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT9.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringT10.eps}
\caption{Coloring of a Simple Matrix}\label{figure:coloring}
\end{figure}

This is easy to perform for small matrices. To cope with larger matrices we
iterate through the matrix for every color and store the position of blocked
nonzero elements for the specific color in a mask.
Each row is compared to the mask and if no nonzero entry of the row interferes
with the already blocked entries, the row is added to the current color and the
mask gets updated accordingly.
As long as there are uncolored rows of the matrix new colors will be used.
To further optimize this we use a mask for 32 colors at once.

\lstset{language=C++, numbers=left, captionpos=b,
        caption={Coloring of a matrix for transposed vector multiplication}}
\begin{lstlisting}
void Coloring ()
  {
    int height = this->Height();
    int width = this->Width();

    Array<int> row_color(height);
    row_color = -1;

    int maxcolor = 0;
    int basecol = 0;

    Array<unsigned int> mask(width);

    int found = 0;

    do
    {
      mask = 0;

      for (int row = 0; row < height; ++row)
      {
        if (row_color[row] >= 0) continue;

        int first = firsti [row];
        int last  = firsti [row+1];

        unsigned check = 0;
        for (int i = first; i < last; ++i)
          check |= mask[colnr[i]];

        if (check != UINT_MAX)
        {
          found++;
          unsigned checkbit = 1;
          int color = basecol;
          while (check & checkbit)
          {
            color++;
            checkbit *= 2;
          }

          row_color[row] = color;
          if (color > maxcolor) maxcolor = color;

          for (int i = first; i < last; ++i)
            mask[colnr[i]] |= checkbit;
        }
      }

      basecol += 8*sizeof(unsigned int); // 32;
    }
    while (found < height);

    Array<int> cntcol(maxcolor+1);
    cntcol = 0;
    for (int row = 0; row < height; ++row)
      ++cntcol[row_color[row]];

    coloring_ = table<int>(cntcol);

    cntcol = 0;
    for (int row = 0; row < height; ++row)
      coloring_[row_color[row]][cntcol[row_color[row]]++] = row;

    std::cout << "needed " << maxcolor+1 << " colors" << std::endl;
  }

\end{lstlisting}

\lstset{language=c++, numbers=left, captionpos=b,
        caption={transposed vector multiplication by coloring a matrix}}
\begin{lstlisting}
void TranMultAdd4 (double s, const BaseVector & x, BaseVector & y)
  {
    FlatVector<double> fx = x.FV<double> ();
    FlatVector<double> fy = y.FV<double> ();

    Coloring();

#pragma omp parallel
    {
      for (auto color : coloring_)
      {
#pragma omp for
        for (int i = 0; i < color.Size(); ++i)
        {
          int first = firsti [color[i]];
          int last  = firsti [color[i]+1];

          for (int j = first; j < last; ++j)
          {
            fy(colnr[j]) += s * data[j] * fx(color[i]);
          }
        }
      }
    }
  }


\end{lstlisting}

\section{Gauß-Seidel-Method}
To solve a linear equation system, we use a special preconditioner, the
Gauß-Seidel-Method. This is similar to the Jacobi-Method, where computing
 the vector $Ax$ is necessary. The difference is,
that we also use the new information optioned by previous computations of
entries of the solution vector. In particular a new entry is given by
$$ x_j^{new} = \frac{1}{A_{jj}} \left(b_{j} - \sum_{k \in K^{new}}A_{jk}
 x_k^{new} - \sum_{k \in K^{old}}A_{jk} x_k^{old}\right)$$
for $j = 1..n$. $K^{new}$ denotes the set of indices of new calculated entries
 of $x$, where $K^{old}$ denotes the old entries.

The order of the calculation is arbitrary, it is not necessary that $x_1$ is
calculated first, $x_2$ second and so on. But the implementing has to make
sure the following is satisfied.

The order of the calculation has to be restorable. In AMG the algorithm is used
to perform a Gauß-Seidel-Preconditioning and (after some prolongation) to do
the same Gauß-Seidel-Preconditioning in reverse order again.

Entries of the output vector $x$ have not to be calculated concurrently if they
depend on each other. Similar to the main problem in chapter \ref{section:trans}  
this would cause wrong entries.

The problems with parallelization are very similar to chapter \ref{section:trans},
 so the ideas for a solution are too.

A coloring alghorithm can be used as well and is similar to implement.
Similar to the transposed matrix vector multiplication we mark the rows which
can run parallel without influencing them.

\subsection{Coloring}

At calculating the product $A_jx$ which modifies the entry $x_j$ (where $A_j$
denotes the j-th row of the matrix $A$) every entry $x_k$ is used, where
 $A_{jk} \neq 0$. According to this, every row $A_j$, which should be added
to an existing color has to have zeroelements on the positions $A_{jk}$ for
all $k$ indices of current rows of this color.

Instead of testing all remaining rows for this property, we use that $A$ is
 symmetric: $A_{kj} = 0 \Leftrightarrow A_{jk} = 0$. We only have to look
at the specific row we added to one color to determine which rows can also
 relate to it.

The main advance of this method is, that the coloring process has to be
 done only once, whereas it is used by GSSmooth and GSSmoothBack as well
(and the order is stored by the process itself, which was a precondition).

For a small matrix the coloring process is demonstrated in figure
\ref{figure:coloringGS}.

\begin{figure}
\includegraphics[width=0.32\textwidth]{graphic/coloringGS1.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringGS4.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringGS7.eps}
\includegraphics[width=0.32\textwidth]{graphic/coloringGS8.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringGS9.eps}\hfill\vline\hfill
\includegraphics[width=0.32\textwidth]{graphic/coloringGS10.eps}
\caption{Coloring of a simple matrix.}\label{figure:coloringGS}
\end{figure}

\subsection{Improve of Speed}

The target is to calculate the inverted matrix $A^{-1}$. As mentioned, the
 AMG-Preconditioner is a combination of Gauß-Seidel and a
projection which coarsens (by multiplying with the transposed
prolongation matrix) the matrix. Theses steps are executed
alternately until the matrix is small enough to invert it directly. Then it is refined to
its original size and Gauß-Seidel is used in the reverse order between these steps.
All together builds one step of the AMG-Preconditioner and many of them
are executed until the iterated matrix is exact enough.

The improve of speed by parallelization is given at these steps (transposed
 multiplication and Gauß-Seidel). The coloring
for both steps has to be done 
separately, but can be used in every step of the AMG.

Figure \ref{figure:sequ} shows a sequential call of AMG, whereas figure \ref{figure:par}
shows a parallel call of AMG. Only one thread is working in the sequential part compared to

\begin{figure}
    \includegraphics[width=1\textwidth]{seq.png}
    \caption{AMG sequential}\label{figure:sequ}
\end{figure}

\begin{figure}
    \includegraphics[width=1\textwidth]{iterations_trace.png}
    \caption{AMG parallel}\label{figure:par}
\end{figure}

\begin{figure}
    \includegraphics[width=1\textwidth]{one_iteration.png}
    \caption{AMG single parallel recursion}
\end{figure}


\lstset{language=C++, numbers=left, captionpos=b, breaklines=true,
        caption={Multiplication Method of the AMG Preconditioner}}
\begin{lstlisting}
void my_AMG_H1 :: Mult (const ngla::BaseVector & b, ngla::BaseVector & x) const
{
  if (inv) {
    x = (*inv) * b;
    return;
  }

  auto residuum = pmat->CreateVector();
  auto coarse_x = coarsemat->CreateVector();
  auto coarse_residuum = coarsemat->CreateVector();

  x = 0;
  jacobi->GSSmooth (x, b);

  if (recAMG) {
    residuum = b - (*pmat) * x;
    coarse_residuum = ngla::Transpose (*prol) * residuum;

    recAMG->Mult(coarse_residuum, coarse_x);

    x += (*prol) * coarse_x;
  }

  jacobi->GSSmoothBack (x, b);
}
\end{lstlisting}

\label{lst:mult}
\begin{figure}
%\input{seminargraph.tex}

\end{figure}

\lstset{language=c++, numbers=left, captionpos=b,
        caption={Gauß-Seidel coloring and balancing}}
\begin{lstlisting}

  void JacobiPrecond<TM,TV_ROW,TV_COL> :: Coloring ()
  {
    //static Timer timer("JacobiPrecond::Coloring");
    //RegionTimer reg (timer);

    int height = mat.Height();
    int width = mat.Width();

    Array<int> row_color(height);
    row_color = -1;

    int maxcolor = 0;
    int basecol = 0;

    Array<unsigned int> mask(width);

    int found = 0;

    int num_threads = task_manager->GetNumThreads();

    do
    {
      mask = 0;

      int block_size = ceil((double)height / (double)num_threads);

      for (int block_row = 0; block_row < block_size; ++block_row)
      {
        for (int threadi = 0; threadi < num_threads; ++threadi)
        {
          int row = block_row + threadi * block_size;
          if (row >= height) break;
          if (row_color[row] >= 0) continue;

          int first = mat.firsti [row];
          int last  = mat.firsti [row+1];

          unsigned check = 0;
          for (int i = first; i < last; ++i)
            check |= mask[mat.colnr[i]];

          if (check != UINT_MAX) // 0xFFFFFFFF)
          {
            found++;
            unsigned checkbit = 1;
            int color = basecol;
            while (check & checkbit)
            {
              color++;
              checkbit *= 2;
            }

            row_color[row] = color;
            if (color > maxcolor) maxcolor = color;

            mask[row] |= checkbit;
          }
        }
      }

      basecol += 8*sizeof(unsigned int); // 32;
    }
    while (found < height);

    Array<int> cntcol(maxcolor+1);
    cntcol = 0;
    for (int row = 0; row < height; ++row)
      ++cntcol[row_color[row]];

    coloring_ = Table<int>(cntcol);

    cntcol = 0;
    for (int row = 0; row < height; ++row)
      coloring_[row_color[row]][cntcol[row_color[row]]++] = row;

    balancing_.SetSize (maxcolor+1);
    for (auto c : Range (balancing_))
    {
      balancing_[c].Calc (coloring_[c].Size(),
        [&] (int bi)
        {
          return 5 + mat.GetRowIndices(coloring_[c][bi]).Size();
        });
    }

    //cout << "Balancing: " << balancing_ << endl;
    std::cout << "needed " << maxcolor+1 << " colors" << std::endl;
  }

\end{lstlisting}

\lstset{language=c++, numbers=left, captionpos=b,
        caption={Gauß-Seidel Smooth}}
\begin{lstlisting}



  void JacobiPrecond<TM,TV_ROW,TV_COL> ::
  GSSmooth (BaseVector & x, const BaseVector & b) const
  {
    //static Timer timer("JacobiPrecond::GSSmooth");
    //timer.AddFlops (mat.NZE());
    //RegionTimer reg (timer);

    FlatVector<TV_ROW> fx = x.FV<TV_ROW> ();
    // dynamic_cast<T_BaseVector<TV_ROW> &> (x).FV();
    const FlatVector<TV_ROW> fb = b.FV<TV_ROW> ();
    // dynamic_cast<const T_BaseVector<TV_ROW> &> (b).FV();

    for (int j = 0; j < this->coloring_.Size(); ++j)
    {
      auto color = this->coloring_[j];
      ParallelFor( this->balancing_[j],
        [this, fx, fb, color] (int i)
        {
          int row = color[i];
          if (!this->inner || this->inner->Test(row))
          {
            TV_ROW ax = mat.RowTimesVector (row, fx);
            fx(row) += invdiag[row] * (fb(row) - ax);
          }
        }, 4);
    }
  }
\end{lstlisting}

\lstset{language=c++, numbers=left, captionpos=b,
        caption={Gauß-Seidel SmoothBack}}
\begin{lstlisting}
  void JacobiPrecond<TM,TV_ROW,TV_COL> ::
  GSSmoothBack (BaseVector & x, const BaseVector & b) const
  {
    //static Timer timer("JacobiPrecond::GSSmoothBack");
    //timer.AddFlops (mat.NZE());
    //RegionTimer reg (timer);

    FlatVector<TV_ROW> fx = x.FV<TV_ROW> ();
    // dynamic_cast<T_BaseVector<TV_ROW> &> (x).FV();
    const FlatVector<TV_ROW> fb = b.FV<TV_ROW> ();
    //dynamic_cast<const T_BaseVector<TV_ROW> &> (b).FV();

    for (int j = this->coloring_.Size()-1; j >= 0; --j)
    {
      auto color = this->coloring_[j];
      ParallelFor( this->balancing_[j],
        [this, fx, fb, color] (int i)
        {
          int row = color[i];
          if (!this->inner || this->inner->Test(row))
          {
            TV_ROW ax = mat.RowTimesVector (row, fx);
            fx(row) += invdiag[row] * (fb(row) - ax);
          }
        }, 4);
    }
  }


\end{lstlisting}



\bibliography{doc}{}
\bibliographystyle{plain}
\end{document}
